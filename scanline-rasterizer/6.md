<div style="text-align:right; color:#aaa">Kristoffer Dyrkorn, March 9, 2025</div>

# Epilogue

(This article is the last part of a [series](./#sections). You can jump to the [previous section](9) if you would like to.)

In this and the previous article series we have looked at two methods to rasterize triangles. One is based on sampling all pixels inside the axis-aligned bounding box of the triangle, and drawing the pixels that lie inside the triangle. The other is based on traversing the triangle edges, calculating start and end coordinates for a series of horizontal lines across the triangle, and then drawing those lines.

But - which of the two methods is _best_?

It all depends on what you need. The Piñeda method (sampling) is very well suited for a runtime environment where parallel exection is simple. Also, the method provides a good separation between the pixel drawing and the interpolation steps, meaning it is easy to support interpolation across multiple attributes registered at the vertices. In the examples here we have only drawn single-color triangles, meaning all vertex colors are equal, but in a given application we might want to do more advanced rendering - ie to register separate colors, normals or uv coordinates (for texture mapping) at each of the triangle corners, and interpolate over those for each pixel. The Piñeda method is very well suited for that.

The scanline conversion method offers a more direct way to draw pixels - it is not based on sampling a region larger than the triangle itself. It will normally require fewer operations, and thus be faster - if running in a single thread.
On my machine (a Macbook Air with an M1 CPU, using Chrome) the scanline rasterizing method is roughly 5 times as fast as the sampling method - when drawing single-color triangles. Whether this actually matters depends on what your needs are.

# Making things faster

If you want to make the scan conversion rasterizer run faster, here are some ideas to try out:

- Right now we scan convert the edges that are shared between triangles twice - one for each triangle. We could cut this work in half by scan converting all edges only once, and looking up the start and end coordinates between pairs of edges as we draw the triangles afterwards.
- At the same time, we must take into consideration that reading or writing to memory is much slower than reading or writing to variables (registers or the stack). The actual performance difference will depend on whether we hit the memory cache or not. So code that accesses memory must be written to be cache-friendly.
- Modern CPUs are heavily pipelined - so branching can be expensive. We need to make the branch prediction logic in the CPU as successful as possible. Meaning, we need to avoid any inner loops containing `if`-statements with randomly varying outcomes.
- However, our runtime platform (JavaScript) makes it hard to control low-level optimizations like this. In the rasterization code I have tried to exploit a feature in Chrome where JavaScript Numbers will be represented - and kept - as native 32-bit integers (so-called Small Integers) if they only store integer values. This will speed up our math operations tremendously - but still is just a internal optimization in Chrome that we have little control over. It would be useful if someone could find a way to ensure (or at least verify) that this optimization kicks in on our code.

Some ideas I have tried out without success, but that should be reconsidered:

- It could be interesting to see whether web workers could help speeding up the Piñeda method. So far calling web workers have too much overhead, and each of them will have to write output back to the pixel buffer via a `SharedArrayBuffer`, which only offers delayed updates or atomic operations, meaning no speedups from parallel execution.
- Writing parts of the rasterizer in WASM could also be useful, but right now it seems there is no way to share a JavaScript typed array with WASM code. This means all changes to pixel buffers done by WASM code would need to be copied back to the JavaScript side, and that has too much overhead in our case.

If you want to have a look at other rasterizers based on scanline conversion, please see:

- The [playdate-dither3d rasterizer written by Aras Pranckevičius](https://github.com/aras-p/playdate-dither3d), based on code in [the classic texture mapping articles by Chris Hecker](https://chrishecker.com/Miscellaneous_Technical_Articles).
- The method published by Kurt Fleischer: "Accurate polygon scan conversion using half-open intervals", from the book "Graphics Gems III". [Here is source code on GitHub](https://github.com/erich666/GraphicsGems/tree/master/gemsiii/accurate_scan), and [here is a paper providing a detailed derivation](https://www.researchgate.net/publication/2249950_Polygon_Scan_Conversion_Derivations) of the method.

<br/>
<br/>
Have fun!

<br/>

Kristoffer

<hr/>

### About the author

Kristoffer Dyrkorn is a Senior Principal Software Engineer at Autodesk. He makes software that renders geodata in architectural visualizations. He can be reached at kristoffer.dyrkorn AT autodesk.com.
