<div style="text-align:right; color:#aaa">Kristoffer Dyrkorn, March 9, 2025</div>

# Epilogue

(This article is the last part of a [series](./#sections). You can jump to the [previous section](9) if you would like to.)

In this and the previous article series we have looked at two methods to rasterize triangles. One is based on first sampling all pixels inside the axis-aligned bounding box of the triangle, and then drawing the pixels that lie inside the triangle. The other is based on first traversing the triangle edges, calculating the start and end coordinates for the horizontal lines, and then drawing those lines.

But - which of the two methods is _best_?

It all depends on what you need. The first method (sampling) is very well suited for a runtime environment where parallel execution is natively supported.

In the examples here we have only drawn single-color triangles, meaning all vertex colors are set to the same values. But, if more advanced rendering is desired, one would need to assign separate colors, normals or uv coordinates (for texture mapping) to each of the triangle vertices. You would then interpolate those values when drawing the pixels. The sampling method makes this kind of interpolation easy to implement.

The scanline conversion method offers a more direct way to draw pixels - it is not based on sampling a region larger than the triangle itself. Instead it works only on the pixels that the triangle covers. This means it will normally require fewer operations, and thus be faster - if running in a single thread. On my machine (a Macbook Air with an M1 CPU, using Chrome) the scanline rasterizing method is roughly 5 times as fast as the sampling method when drawing single-color triangles. Whether this actually matters depends on what your needs are. But supporting interpolation of many values is more complex to implement here.

# Making things even faster

As mentioned, a software rasterizer will never be fast enough. Here is an idea to try out that might improve the performance. I included a couple of caveats as well, to show the complexity we might have to deal with. The only way to know what actually works is to try out things and measure the effects.

Right now the scan conversion of edges that are shared between triangles happens twice, one for each of the two triangles. We could cut the scan conversion work roughly in half by looping through all edges and scan converting them only once. We would then need to look up the right pairs of edges (and their respective start and end coordinates) as we draw each of the triangles afterwards.

At the same time, we must take into consideration that reading or writing to memory is much slower than reading or writing to variables (registers or the stack). The actual performance difference will depend on whether we hit the memory cache or not. So code that accesses memory must be made cache-friendly.

Also, modern CPUs are heavily pipelined - so branching can be expensive. We need to make the branch prediction logic in the CPU as successful as possible. Meaning, we need to avoid any inner loops containing `if`-statements with randomly varying outcomes.

However, our runtime platform (JavaScript) makes it hard to control low-level optimizations like this. In the rasterization code I have tried to exploit a feature in Chrome where JavaScript Numbers will be represented - and kept - as native 32-bit integers (so-called Small Integers) as long as they only contain integer values. This will speed up our math operations tremendously - but it still is an internal optimization in Chrome that we have little control over. It would be useful if there was a way to ensure (or at least verify) that this optimization happens when running our code.

Here are some additional ideas I have tried out, without success, but that could be reconsidered:

- It would be interesting to see whether web workers (parallelism) could help speeding up the sampling method. So far calling web workers has too much overhead, and each of the web workers will have to write its output back to the pixel buffer via a `SharedArrayBuffer`. This buffer only offers delayed updates from workers, or atomic operations, which would mean no speedup gains from parallel execution.
- Writing parts of the rasterizer in WASM could also be useful, but right now it seems there is no way to share a JavaScript typed array with WASM code. Instead, all changes to pixel buffers done by WASM code seems to have to be copied back to the JavaScript side - and that has too much overhead in our case.

If you want to have a look at other rasterizers based on scanline conversion, please check out the [playdate-dither3d rasterizer written by Aras Pranckeviƒçius](https://github.com/aras-p/playdate-dither3d), based on [the classic texture mapping articles by Chris Hecker](https://chrishecker.com/Miscellaneous_Technical_Articles). Or, Kurt Fleischer's article "Accurate polygon scan conversion using half-open intervals", from the book "Graphics Gems III". There is [source code](https://github.com/erich666/GraphicsGems/tree/master/gemsiii/accurate_scan) available on GitHub, and [a paper](https://www.researchgate.net/publication/2249950_Polygon_Scan_Conversion_Derivations) providing a detailed explanation of the algorithms.

<br/>
<br/>
Have fun!

<br/>

Kristoffer

<hr/>

### About the author

Kristoffer Dyrkorn is a Senior Principal Software Engineer at Autodesk. He makes software that renders geodata in architectural visualizations. He can be reached at kristoffer.dyrkorn AT autodesk.com.
